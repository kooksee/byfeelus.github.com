<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>多进程下载器 on 百里求一的博客</title>
        <link>http://kooksee.github.io/tags/%E5%A4%9A%E8%BF%9B%E7%A8%8B%E4%B8%8B%E8%BD%BD%E5%99%A8/</link>
        <language>zh-CN</language>
        <author>CoderZh</author>
        <rights>Copyright (c) 2015, CoderZh; all rights reserved.</rights>
        <updated>Mon, 01 Jan 0001 00:00:00 UTC</updated>
        
        <item>
            <title>python多进程断点续传分片下载器</title>
            <link>http://kooksee.github.io/blog/python%E5%A4%9A%E8%BF%9B%E7%A8%8B%E6%96%AD%E7%82%B9%E7%BB%AD%E4%BC%A0%E5%88%86%E7%89%87%E4%B8%8B%E8%BD%BD%E5%99%A8/</link>
            <pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate>
            <author>CoderZh</author>
            <guid>http://kooksee.github.io/blog/python%E5%A4%9A%E8%BF%9B%E7%A8%8B%E6%96%AD%E7%82%B9%E7%BB%AD%E4%BC%A0%E5%88%86%E7%89%87%E4%B8%8B%E8%BD%BD%E5%99%A8/</guid>
            <description>&lt;p&gt;因为爬虫要用到下载器，但是直接用urllib下载很慢，所以找了很久终于找到一个让我欣喜的下载器。他能够断点续传分片下载，极大提高下载速度。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#! /usr/bin/env python
# encoding=utf-8

from __future__ import unicode_literals

from multiprocessing.dummy import Pool as ThreadPool
import threading

import os
import sys
import cPickle
from collections import namedtuple
import urllib2
from urlparse import urlsplit

import time


# global lock
lock = threading.Lock()


# default parameters
defaults = dict(
    thread_count=10,
    buffer_size=500 * 1024,
    block_size=1000 * 1024)


def progress(percent, width=50):
    print &amp;quot;%s %d%%\r&amp;quot; % ((&#39;%%-%ds&#39; % width) % (width * percent / 100 * &#39;=&#39;), percent),
    if percent &amp;gt;= 100:
        print
        sys.stdout.flush()


def write_data(filepath, data):
    with open(filepath, &#39;wb&#39;) as output:
        cPickle.dump(data, output)


def read_data(filepath):
    with open(filepath, &#39;rb&#39;) as output:
        return cPickle.load(output)


FileInfo = namedtuple(&#39;FileInfo&#39;, &#39;url name size lastmodified&#39;)


def get_file_info(url):
    class HeadRequest(urllib2.Request):

        def get_method(self):
            return &amp;quot;HEAD&amp;quot;
    res = urllib2.urlopen(HeadRequest(url))
    res.read()
    headers = dict(res.headers)
    size = int(headers.get(&#39;content-length&#39;, 0))
    lastmodified = headers.get(&#39;last-modified&#39;, &#39;&#39;)
    name = None
    if headers.has_key(&#39;content-disposition&#39;):
        name = headers[&#39;content-disposition&#39;].split(&#39;filename=&#39;)[1]
        if name[0] == &#39;&amp;quot;&#39; or name[0] == &amp;quot;&#39;&amp;quot;:
            name = name[1:-1]
    else:
        name = os.path.basename(urlsplit(url)[2])

    return FileInfo(url, name, size, lastmodified)


def download(url, output,
             thread_count=defaults[&#39;thread_count&#39;],
             buffer_size=defaults[&#39;buffer_size&#39;],
             block_size=defaults[&#39;block_size&#39;]):
    # get latest file info
    file_info = get_file_info(url)

    # init path
    if output is None:
        output = file_info.name
    workpath = &#39;%s.ing&#39; % output
    infopath = &#39;%s.inf&#39; % output

    # split file to blocks. every block is a array [start, offset, end],
    # then each greenlet download filepart according to a block, and
    # update the block&#39; offset.
    blocks = []

    if os.path.exists(infopath):
        # load blocks
        _x, blocks = read_data(infopath)
        if (_x.url != url or
                _x.name != file_info.name or
                _x.lastmodified != file_info.lastmodified):
            blocks = []

    if len(blocks) == 0:
        # set blocks
        if block_size &amp;gt; file_info.size:
            blocks = [[0, 0, file_info.size]]
        else:
            block_count, remain = divmod(file_info.size, block_size)
            blocks = [[i * block_size, i * block_size,
                       (i + 1) * block_size - 1] for i in range(block_count)]
            blocks[-1][-1] += remain
        # create new blank workpath
        with open(workpath, &#39;wb&#39;) as fobj:
            fobj.write(&#39;&#39;)

    print &#39;Downloading %s&#39; % url
    # start monitor
    threading.Thread(target=_monitor, args=(
        infopath, file_info, blocks)).start()

    # start downloading
    with open(workpath, &#39;rb+&#39;) as fobj:
        args = [(url, blocks[i], fobj, buffer_size)
                for i in range(len(blocks)) if blocks[i][1] &amp;lt; blocks[i][2]]

        if thread_count &amp;gt; len(args):
            thread_count = len(args)

        pool = ThreadPool(thread_count)
        pool.map(_worker, args)
        pool.close()
        pool.join()

    # rename workpath to output
    if os.path.exists(output):
        os.remove(output)
    os.rename(workpath, output)

    # delete infopath
    if os.path.exists(infopath):
        os.remove(infopath)

    assert all([block[1] &amp;gt;= block[2] for block in blocks]) is True


def _worker((url, block, fobj, buffer_size)):
    req = urllib2.Request(url)
    req.headers[&#39;Range&#39;] = &#39;bytes=%s-%s&#39; % (block[1], block[2])
    res = urllib2.urlopen(req)

    while 1:
        chunk = res.read(buffer_size)
        if not chunk:
            break
        with lock:
            fobj.seek(block[1])
            fobj.write(chunk)
            block[1] += len(chunk)


def _monitor(infopath, file_info, blocks):
    while 1:
        with lock:
            percent = sum([block[1] - block[0]
                           for block in blocks]) * 100 / file_info.size
            progress(percent)
            if percent &amp;gt;= 100:
                break
            write_data(infopath, (file_info, blocks))
        time.sleep(2)


if __name__ == &#39;__main__&#39;:
    import argparse
    parser = argparse.ArgumentParser(description=&#39;多线程文件下载器.&#39;)
    parser.add_argument(&#39;url&#39;, type=str, help=&#39;下载连接&#39;)
    parser.add_argument(&#39;-o&#39;, type=str, default=None,
                        dest=&amp;quot;output&amp;quot;, help=&#39;输出文件&#39;)
    parser.add_argument(
        &#39;-t&#39;, type=int, default=defaults[&#39;thread_count&#39;], dest=&amp;quot;thread_count&amp;quot;, help=&#39;下载的线程数量&#39;)
    parser.add_argument(
        &#39;-b&#39;, type=int, default=defaults[&#39;buffer_size&#39;], dest=&amp;quot;buffer_size&amp;quot;, help=&#39;缓存大小&#39;)
    parser.add_argument(
        &#39;-s&#39;, type=int, default=defaults[&#39;block_size&#39;], dest=&amp;quot;block_size&amp;quot;, help=&#39;字区大小&#39;)

    argv = sys.argv[1:]

    if len(argv) == 0:
        argv = [&#39;https://eyes.nasa.gov/eyesproduct/EYES/os/win&#39;]

    args = parser.parse_args(argv)

    start_time = time.time()
    download(args.url, args.output, args.thread_count,
             args.buffer_size, args.block_size)
    print &#39;下载时间: %ds&#39; % int(time.time() - start_time)

&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        
    </channel>
</rss>
